{
 "metadata": {
  "name": "",
  "signature": "sha256:dcee93dfa6ccf74a385ae5445d85494993e838bdf1a4705e15f1b35af4b9007f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import requests, urllib2, re\n",
      "from pprint import pprint\n",
      "from ete2 import Tree\n",
      "import collections\n",
      "import itertools\n",
      "import time\n",
      "import nltk\n",
      "from nltk import FreqDist\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "stopwords.extend(['We', 'The', 'due', 'physics', '.', ',', 'also', 'using', 'may', 'discuss', 'demonstrate', 'show', 'states', 'regime'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getSiteData(url):\n",
      "    # return article related info from the souped site data\n",
      "    extract = {}\n",
      "    response = requests.get(url)\n",
      "    extract['soup'] = BeautifulSoup(response.text, 'html.parser')\n",
      "    extract['citations link'] = None\n",
      "    try:\n",
      "        extract['Article name'] = extract['soup'].find(\"meta\", {\"name\" : \"citation_title\"})['content']\n",
      "    except Exception as e:\n",
      "        pass\n",
      "    try: \n",
      "        extract['Authors'] = extract['soup'].find(\"meta\", {\"name\" : \"citation_authors\"})['content']       \n",
      "    except Exception as e:\n",
      "        pass\n",
      "    try: \n",
      "        extract['Journal Title'] = extract['soup'].find(\"meta\", {\"name\" : \"citation_journal_title\"})['content']       \n",
      "    except Exception as e:\n",
      "        pass\n",
      "    try: \n",
      "        extract['Keywords'] = extract['soup'].find(\"meta\", {\"name\" : \"citation_keywords\"})['content']       \n",
      "    except Exception as e:\n",
      "        pass\n",
      "    try: \n",
      "        extract['arXiv link'] = extract['soup'].findAll('a', href=re.compile('[&amp;]link_type=PREPRINT'))[0].get('href')       \n",
      "    except Exception as e:\n",
      "        pass\n",
      "    try: \n",
      "        extract['citations link'] = extract['soup'].findAll('a', href=re.compile('[&amp;]link_type=CITATIONS'))[0].get('href')       \n",
      "    except Exception as e:\n",
      "        pass\n",
      "    \n",
      "    return extract"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getAbstract2(url):\n",
      "    # trying to get abstract from ads page : painful as it has no tags!\n",
      "    response = requests.get(url)\n",
      "    extract = BeautifulSoup(response.text, 'html.parser')\n",
      "    abs = extract.findAll('h3', align=re.compile('center')).contents\n",
      "    return abs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getAbstract(url):\n",
      "    # return abstract from the extracted site data\n",
      "    extracted = getSiteData(url)\n",
      "    return extracted['soup'].blockquote.contents[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def findNamedEnt(tokens):\n",
      "    tagged = nltk.pos_tag(tokens)\n",
      "    return nltk.ne_chunk(tagged, binary = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def createTree(tokensDict, print_Tree = True):\n",
      "    elems = str(tuple([int(w) for w in tokensDict.keys()])) + ';'\n",
      "    tr = Tree(elems)\n",
      "    for leaf in tr:\n",
      "        leaf.add_features(infom = tokensDict.get(leaf.name, 'name'))\n",
      "    if print_Tree:\n",
      "        print tr.get_ascii(attributes = ['name', 'infom'], show_internal= False)\n",
      "    return tr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arXivData(url):\n",
      "    # return article related info from the souped site data\n",
      "    extract = {}\n",
      "    response = requests.get(url)\n",
      "    extract['soup'] = BeautifulSoup(response.text, 'html.parser')\n",
      "    try:\n",
      "        extract['Article name'] = extract['soup'].find(\"meta\", {\"name\" : \"citation_title\"})['content']\n",
      "        extract['Authors'] = extract['soup'].find_all(\"meta\", {\"name\" : \"citation_author\"})\n",
      "        extract['Journal Title'] = extract['soup'].find(\"meta\", {\"name\" : \"citation_journal_title\"})['content']\n",
      "        extract['Abstract'] = extract['soup'].blockquote.contents[2]\n",
      "        extract['arXiv id'] = extract['soup'].find(\"meta\", {\"name\" : \"citation_arxiv_id\"})['content']\n",
      "    except Exception as e:\n",
      "        pass\n",
      "    \n",
      "    return extract"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def arXivAds(url):\n",
      "    # use arXiv link to goto ads\n",
      "    arXiv_id = url.split('abs/')[1]\n",
      "    ads_search = requests.get('http://adsabs.harvard.edu/cgi-bin/basic_connect?qsearch=arxiv:' + arXiv_id)\n",
      "    extract = BeautifulSoup(ads_search.text, 'html.parser')\n",
      "    adslink = extract.findAll('a', href=re.compile('http://adsabs.harvard.edu/cgi-bin/nph-data_query?'))[0].get('href')\n",
      "    return adslink"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def citeAuthors(url):\n",
      "    # get names of last authors of papers citing the main \n",
      "    preprint = getSiteData(url)\n",
      "    lastAuts = [] ; titl = []; autLink = None\n",
      "    try:\n",
      "    \ttitl = preprint['Article name']\n",
      "    except Exception as e:\n",
      "        pass\n",
      "    if preprint['citations link'] is not None:\n",
      "        citeData = getSiteData(preprint['citations link']) # soup of the citations link page\n",
      "        sublinks = citeData['soup'].findAll('a', href = re.compile('link_type=ABSTRACT')) # get links of individual citations\n",
      "        sublink = [sublinks[w].get('href') for w in range(0, len(sublinks))] \n",
      "        sub = list(set(sublink)) # keeping only unique links to the citation pages\n",
      "        citingAuthors = [''.join(node.findAll(text=True)) for node in citeData['soup'].findAll('td', align=re.compile('left'), valign=re.compile('top'), width=re.compile('25%'))]\n",
      "        lastAuts = [0]*len(citingAuthors)\n",
      "        for w in xrange(0, len(citingAuthors)):\n",
      "            try:\n",
      "                lastAuts[w] = (citingAuthors[w].split(';')[-1]).encode('utf-8')  # to change values from unicode to string // .encode() is for special characters\n",
      "            except Exception as e:\n",
      "                print e\n",
      "            pass\n",
      "        autLink = zip(lastAuts, sub) # list of tuples of lastAuthors and links to those articles\n",
      "    return titl, lastAuts, autLink"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def citnAutAbs(url):\n",
      "    # get names of last authors of papers citing the main \n",
      "    preprint = getSiteData(url)  # soup of the main adspage\n",
      "    lastAuts = [] ; titl = []; subsoups = []; titles = []; autLink = None\n",
      "    try:\n",
      "    \ttitl = preprint['Article name']\n",
      "    except Exception as e:\n",
      "        pass\n",
      "    if preprint['citations link'] is not None:\n",
      "        # if citations_link exits go to it\n",
      "        citeData = getSiteData(preprint['citations link']) # soup of the citations page /returns 'soup', 'citations_link', 'arXiv link'||Need Soup\n",
      "        sublinks = citeData['soup'].findAll('a', href = re.compile('link_type=ABSTRACT')) # get links of individual citations\n",
      "        sublink = [sublinks[w].get('href') for w in range(0, len(sublinks))] \n",
      "        sublink = list(set(sublink)) # keeping only unique links of the citations\n",
      "        # get author names of citations:\n",
      "        citingAuthors = [''.join(node.findAll(text=True)) for node in citeData['soup'].findAll('td', align=re.compile('left'), valign=re.compile('top'), width=re.compile('25%'))]\n",
      "        lastAuts = [0]*len(citingAuthors)\n",
      "        for w in xrange(0, len(citingAuthors)):\n",
      "            try:\n",
      "                lastAuts[w] = (citingAuthors[w].split(';')[-1]).encode('utf-8')  # to change values from unicode to string // .encode() is for special characters\n",
      "            except Exception as e:\n",
      "                print e\n",
      "            pass\n",
      "        # get abstracts of all sublinks\n",
      "        subsoups = [getSiteData(sublink[w]) for w in range(0, len(sublink)) ]\n",
      "        abstracts = [0]*len(sublink)\n",
      "        subtitles = [0]*len(sublink)\n",
      "        \n",
      "        for w in xrange(0, len(sublink)):\n",
      "            try:\n",
      "                abstracts[w] = getAbstract(subsoups[w]['arXiv link']) \n",
      "            except Exception as e:\n",
      "                print e\n",
      "            pass\n",
      "        subtitles = [subsoups[w]['Article name'] for w in range(0, len(sublink))]\n",
      "        \n",
      "        #abstracts = [ getAbstract(subsoups[w]['arXiv link']) for w in range(0, len(sublink)) if 'arXiv link' in subsoups[w] ]\n",
      "        #autLink = zip(lastAuts, sublink) # list of tuples of lastAuthors and links to those articles : why do this?\n",
      "    return titl, lastAuts, sublink, abstracts, subtitles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "t = time.time()\n",
      "titl, authors, links, abst, subtitls = citnAutAbs(url)\n",
      "print time.time() - t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'arXiv link'\n",
        "32.3860859871"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print abst[3]\n",
      "print len(links), len(abst), abst[2], '\\n', subtitls[6], type(abst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "12 12  We propose the use of Rydberg interactions and ensembles of cold atoms in\n",
        "mixed state for the implementation of a protocol for deterministic quantum\n",
        "computation with one quantum bit (DQC1) that can be readily operated in high\n",
        "dimensional Hilbert spaces. We propose an experimental test for the scalability\n",
        "of the protocol and to study the physics of discord. Furthermore we develop a\n",
        "scheme to add control to non-trivial unitaries that will enable the study of\n",
        "many-body physics with ensembles in mixed states.\n",
        "\n",
        "Nonlinear Optics Using Cold Rydberg Atoms\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tgs = get_tags('0') #abst[5])\n",
      "Ent = extract_entity_names(tgs)\n",
      "print Ent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def auts(url):\n",
      "    if url is not None:\n",
      "        url = arXivAds(url)\n",
      "        artName, autList, autLink , abstracts, titles = citnAutAbs(url)\n",
      "        str_ar = []; Ent = [0]*len(abstracts); Ent_ti = [0]*len(abstracts);\n",
      "        ### Get entity name from abstracts    \n",
      "        for w in xrange(0, len(abstracts)):\n",
      "            tgs = get_tags(str(abstracts[w]))\n",
      "            tgs2 = get_tags(str(titles[w]))\n",
      "            # specific type\n",
      "            Ent[w] = extract_entity_names(tgs)\n",
      "            Ent_ti[w] = extract_entity_names(tgs2)\n",
      "        Ent = set([item for sublist in Ent for item in sublist])\n",
      "        Ent_ti = set([item for sublist in Ent_ti for item in sublist])    \n",
      "        no_cite = \"<li> <span><i class='icon-leaf'></i> No citations! </span><a href=\"\"> Goes somewhere</a> </li>\"\n",
      "        if len(autList):\n",
      "            for k, item in enumerate(autList):\n",
      "                if item is not None:\n",
      "                    str_ar.append(\"<li> <span><i class='icon-leaf'></i> \" + item + \" </span><a href=\"\"> Goes somewhere</a> </li>\") \n",
      "            return str(artName), str(len(str_ar)), \"\".join(str_ar), Ent, Ent_ti\n",
      "        else:\n",
      "            return str(artName), str(len(str_ar)), no_cite, Ent, Ent_ti\n",
      "    else:\n",
      "        return None, None, None, None, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def auts2L(url):\n",
      "    if url is not None:\n",
      "        url = arXivAds(url)\n",
      "        artName, autList, autLink, abstracts, titles = citeAuthors(url)\n",
      "        str_ar = []\n",
      "        no_cite = \"<li> <span><i class='icon-leaf'></i> No citations! </span><a href=\"\"> Goes somewhere</a> </li>\"\n",
      "        if len(autList):\n",
      "            br = []\n",
      "            br = [(citeAuthors(autLink[w][1])) for w in range(len(autLink))] # Level 2 citations\n",
      "            alls = [br[w][1] for w in range(len(br))] # putting together all Level 2 author names\n",
      "            alls = list(autList) + list(itertools.chain(*alls)) \n",
      "            uniques = [alls.count(w) for w in set(alls)] # count num. of times unique author names appear\n",
      "            for k, item in enumerate(autList):\n",
      "                sub_str = str()\n",
      "                if item is not None:\n",
      "                    for j, subitem in enumerate(br[k][1]):\n",
      "                        if subitem is not None:\n",
      "                            sub_str = sub_str + \"<li> <span><i class='icon-leaf'></i> \" + subitem + \" </span><a href=\"\"> Goes somewhere</a> </li>\"\n",
      "                    if len(br[k][1]):\n",
      "                        sub_str = \"<ul>\" + sub_str + \"</ul>\"\n",
      "                    str_ar.append(\"  <li> <span><i class='icon-leaf'></i> \" + item + \" </span><a href=\"\"> Goes somewhere</a> \"\n",
      "                              + sub_str + \"</li>\" )\n",
      "            return str(artName), str(len(str_ar)), \"\".join(str_ar)\n",
      "        else:\n",
      "            return str(artName), str(len(str_ar)), no_cite\n",
      "    else:\n",
      "        return None, None, None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_url = 'http://arxiv.org/abs/1203.6764' #'http://arxiv.org/abs/1210.5657' #http://arxiv.org/abs/1210.6025'  # http://adsabs.harvard.edu/abs/2014RvMP...86..153G\n",
      "url = arXivAds(root_url)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "st = time.time()\n",
      "level1 = auts(root_url)\n",
      "print time.time() - st"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'arXiv link'\n",
        "36.8095920086"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print level1[3], level1[4] #too much ////\n",
      "#set([item for sublist in level1[3] for item in sublist])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['EIT', 'Rydberg', 'DQC1', 'Laplace', 'Rabi', 'Hilbert']) set(['Control', 'Quantum Optics', 'Nonlinear Optics Using Cold Rydberg Atoms', 'Rydberg', 'Photon', 'Storage', 'Microwave', 'Optical Photons Using Rydberg Polaritons', 'Rydberg Media', 'Spatial', 'Emergence', 'Full Counting Statistics', 'Hilbert', 'Phase Diagram'])\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "level2 = auts2L(root_url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titl, las, citLink = citeAuthors(url) # getting title, last_authornames, and citation_links of main url\n",
      "br = []\n",
      "start_time = time.time()\n",
      "br = [(citeAuthors(citLink[w][1])) for w in range(len(citLink))] # for each citation_link above do a citeAuthors\n",
      "print titl, '\\n', time.time() - start_time, 'seconds'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Generating non-Gaussian states using collisions between Rydberg polaritons \n",
        "56.1356458664 seconds\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "alls = [br[w][1] for w in range(len(br))] # putting together all sub author names \n",
      "allauts = list(las) + list(itertools.chain(*alls)) # eveythin together\n",
      "uniques = [allauts.count(w) for w in set(allauts)] # counting how many times each authorname appears in 'alls'\n",
      "uniq = dict(zip(set(allauts), uniques)) # dict of unique authorname, no. of times it appears \n",
      "uniq = dict((key,value) for key, value in uniq.iteritems() if value >2) # keeping only frequently occurring authornames\n",
      "print len(br[1]), len(alls), len(uniques), len(uniq), '\\n', uniq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 12 82 29 \n",
        "{' Huang,\\xc2\\xa0Guoxiang': 3, ' Garrahan,\\xc2\\xa0Juan\\xc2\\xa0P.': 4, ' Nikolopoulos,\\xc2\\xa0Georgios\\xc2\\xa0M.': 3, ' Weidem\\xc3\\xbcller,\\xc2\\xa0M.': 6, ' Raithel,\\xc2\\xa0Georg': 3, ' Evers,\\xc2\\xa0J\\xc3\\xb6rg': 14, ' Bergamini,\\xc2\\xa0S.': 3, ' Lesanovsky,\\xc2\\xa0I.': 3, ' Grangier,\\xc2\\xa0P.': 3, ' Molmer,\\xc2\\xa0Klaus': 3, ' Adams,\\xc2\\xa0C.\\xc2\\xa0S.': 8, ' Fleischhauer,\\xc2\\xa0Michael': 4, ' M\\xc3\\xb8lmer,\\xc2\\xa0Klaus': 5, ' Chan,\\xc2\\xa0Ching-Kit': 3, ' Kuzmich,\\xc2\\xa0A.': 6, ' Lesanovsky,\\xc2\\xa0Igor': 9, ' Pohl,\\xc2\\xa0Thomas': 6, ' L\\xc3\\xb6w,\\xc2\\xa0R.': 4, ' Adams,\\xc2\\xa0Charles\\xc2\\xa0S.': 3, ' Simon,\\xc2\\xa0Christoph': 3, ' Pohl,\\xc2\\xa0T.': 7, ' Xiao,\\xc2\\xa0Min': 3, ' Grangier,\\xc2\\xa0Philippe': 8, ' Zoller,\\xc2\\xa0Peter': 5, ' Vuletic,\\xc2\\xa0Vladan': 3, ' Wu,\\xc2\\xa0Jin-Hui': 4, ' Morsch,\\xc2\\xa0O.': 3, ' Ates,\\xc2\\xa0C.': 3, ' Weatherill,\\xc2\\xa0K.\\xc2\\xa0J.': 3}\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Debugging : "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "br = []\n",
      "for w in range(8,10):\n",
      "    print w, cites[w][1]\n",
      "    br = citeAuthors(cites[w][1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8 http://adsabs.harvard.edu/cgi-bin/nph-data_query?bibcode=2008PhRvL.100b4103D&db_key=PHY&link_type=ABSTRACT\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://adsabs.harvard.edu/cgi-bin/nph-data_query?bibcode=2013arXiv1310.7854D&db_key=PRE&link_type=ABSTRACT\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(alls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "201"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- keywords applicable to individual uniq elements (tokenize related abstracts)\n",
      "- return time taken by each level\n",
      "- return no. of citations by individual authors\n",
      "- above a min. number of author threshold spit out keywords and titles"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Author tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get list of all citing last authors\n",
      "start_time = time.time()\n",
      "autNames = citeAuthors(root_url)\n",
      "print time.time() - start_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.53891205788\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Create dictionary with (citation_no : last author) pairs;  if-else since some matchwords maybe empty \n",
      "\n",
      "autList = [(str(w), ''.join(list(zip(*autNames[w]))[0]) ) if autNames[w] else (str(w), '') for w in xrange(0, len(autNames)) ]\n",
      "autDict = dict(autList)\n",
      "#autDict.update((x, (y)) for x, y in autDict.items()) # to change values from unicode to string // .encode() is for special characters\n",
      "print autDict, '\\n'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'11': u' Kuzmich,\\xa0A.', '10': u' Kennedy,\\xa0T.\\xa0A.\\xa0B.', '1': u' Morsch,\\xa0O.', '0': u' Weidem\\xfcller,\\xa0M.', '3': u' Adams,\\xa0C.\\xa0S.', '2': u' Bergamini,\\xa0S.', '5': u' Grangier,\\xa0Philippe', '4': u' Adams,\\xa0Charles\\xa0S.', '7': u' Adams,\\xa0C.\\xa0S.', '6': u' Pohl,\\xa0Thomas', '9': u' Kuzmich,\\xa0A.', '8': u' Sun,\\xa0C.\\xa0P.'} \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Treedict of last author names ###\n",
      "\n",
      "createTree(autDict, print_Tree=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   /-11,  Kuzmich,\u00a0A.\n",
        "  |\n",
        "  |--10,  Kennedy,\u00a0T.\u00a0A.\u00a0B.\n",
        "  |\n",
        "  |--1,  Morsch,\u00a0O.\n",
        "  |\n",
        "  |--0,  Weidem\u00fcller,\u00a0M.\n",
        "  |\n",
        "  |--3,  Adams,\u00a0C.\u00a0S.\n",
        "  |\n",
        "  |--2,  Bergamini,\u00a0S.\n",
        "--|\n",
        "  |--5,  Grangier,\u00a0Philippe\n",
        "  |\n",
        "  |--4,  Adams,\u00a0Charles\u00a0S.\n",
        "  |\n",
        "  |--7,  Adams,\u00a0C.\u00a0S.\n",
        "  |\n",
        "  |--6,  Pohl,\u00a0Thomas\n",
        "  |\n",
        "  |--9,  Kuzmich,\u00a0A.\n",
        "  |\n",
        "   \\-8,  Sun,\u00a0C.\u00a0P.\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "Tree node 'NoName' (0x7f36b32d719)"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "######  NLTK  #########"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Longest common-word thread\n",
      "## Trends : tree of last-author citation histories -- get common thread\n",
      "\n",
      "## Use guidewords such as (show, prove, study, obtain, demonstrate, etc.) in regex, chunking etc.\n",
      "## pdf scrape \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 325
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tags(txt):\n",
      "    # returns part of speech tags for input text\n",
      "    tokens = nltk.word_tokenize(txt)\n",
      "    return nltk.pos_tag(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_entity_names(tagged):\n",
      "    # returns named entity for input tagged text\n",
      "    namedEnt = nltk.ne_chunk(tagged, binary = True)\n",
      "    NE = []\n",
      "    for c in namedEnt:\n",
      "      if hasattr(c, 'label'):\n",
      "        NE.append(' '.join(i[0] for i in c.leaves()))\n",
      "    return NE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mainAbs = getAbstract(getSiteData(url)['arXiv link'])\n",
      "print 'Abstract : \\n', mainAbs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Abstract : \n",
        " We investigate theoretically the deterministic generation of quantum states\n",
        "with negative Wigner functions, by using giant non-linearities due to\n",
        "collisional interactions between Rydberg polaritons. The state resulting from\n",
        "the polariton interactions may be transferred with high fidelity into a\n",
        "photonic state, which can be analyzed using homodyne detection followed by\n",
        "quantum tomography. Besides generating highly non-classical states of the\n",
        "light, this method can also provide a very sensitive probe for the physics of\n",
        "the collisions involving Rydberg states.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mainTags = get_tags(mainAbs)\n",
      "# Get Named Entity lists for main abstract\n",
      "mainEnt = extract_entity_names(mainTags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print mainEnt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Rydberg', u'Rydberg']\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root_url = 'http://arxiv.org/abs/1203.6764'\n",
      "url = arXivAds(root_url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This block is to get infos of all citing articles for nltk analysis below\n",
      "citeData = getSiteData(getSiteData(url)['citations link'])\n",
      "# get preprint links from citation page\n",
      "sub_link = citeData['soup'].findAll('a', href=re.compile('link_type=ABSTRACT')) \n",
      "cites = set([sub.get('href') for sub in sub_link]) # get only unique links with link_type = ABSTRACT\n",
      "# Extract info from all the links above\n",
      "citation_info = [getSiteData(cit) for cit in cites]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titl = citation_info[3]['Article name'] #getAbstract(citation_info[2]['arXiv link'])\n",
      "txt = getAbstract(citation_info[3]['arXiv link'])\n",
      "print txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " We use a microwave field to control the quantum state of optical photons\n",
        "stored in a cold atomic cloud. The photons are stored in highly excited\n",
        "collective states (Rydberg polaritons) enabling both fast qubit rotations and\n",
        "control of photon-photon interactions. Through the collective read-out of these\n",
        "pseudo-spin rotations it is shown that the microwave field modifies the\n",
        "long-range interactions between polaritons. This technique provides a powerful\n",
        "interface between the microwave and optical domains, with applications in\n",
        "quantum simulations of spin liquids, quantum metrology and quantum networks.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Get entity name from some text\n",
      "    \n",
      "tgs = get_tags(txt)\n",
      "# non-stopwords\n",
      "nonstop = [x for x in tgs if x[0] not in stopwords]\n",
      "# specific type\n",
      "Ent = extract_entity_names(tgs)\n",
      "print titl, Ent, '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Storage and Control of Optical Photons Using Rydberg Polaritons [u'Rydberg'] \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#grammar = \"NP: {<DT>?<JJ>*<NN>}\" # define noun phrase : optional determiner (DT) followed by any no. of adj. (JJ) & then noun (NN)\n",
      "#grammar = \"NP: {<JJ>*<NNS>+} \" # ---->   * :0 or more ; + :1 or more\n",
      "grammar = \"NP: {<DT>?<JJ>*<NNS>?<NN>?<NNS>?}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "result = cp.parse(tgs)\n",
      "print result\n",
      "#result.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP Dispersive/JJ optical/JJ nonlinearities/NNS)\n",
        "  in/IN\n",
        "  (NP a/DT)\n",
        "  Rydberg/NNP\n",
        "  electromagnetically-induced-transparency/NNP\n",
        "  (NP medium/NN))\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filt(x):\n",
      "    return x.label()=='NP'\n",
      "nps = []\n",
      "for subtree in result.subtrees(filter =  filt): # Generate all subtrees\n",
      "    print subtree, len(subtree)\n",
      "    if len(subtree)>1:\n",
      "        nps.append(subtree)\n",
      "print nps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(NP Dispersive/JJ optical/JJ nonlinearities/NNS) 3\n",
        "(NP a/DT) 1\n",
        "(NP medium/NN) 1\n",
        "[Tree('NP', [(u'Dispersive', 'JJ'), (u'optical', 'JJ'), (u'nonlinearities', 'NNS')])]\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get data from citations link\n",
      "citeData = getSiteData(prep['citations link'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Frequency distribution of words #\n",
      "vocabulary = fdist.keys()\n",
      "freqs = fdist.values()\n",
      "print vocabulary[:3], freqs[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'the', u',', u'of'] [7, 6, 6]\n"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.help.upenn_tagset('JJ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "JJ: adjective or numeral, ordinal\n",
        "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
        "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
        "    multilingual multi-disciplinary ...\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titleList = [(str(w), [citation_info[w]['Article name'], neDict.values()[w]]) for w in xrange(0,len(allAbs))]\n",
      "titleList = dict(titleList)\n",
      "orderedDict = collections.OrderedDict(sorted(titleList.items()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}